# Imports
library(dplyr)
library(rsample)
library(tidymodels)
library(lubridate)
library(caret)
install.packages("glmnet")
library(glmnet)
install.packages("dials")
install.packages("readr")
library(dials)
library(readr)
install.packages("vip")
library(vip)
# Read the CSV file
rental <- read_csv("C:/Users/Anthony Ebuboh/Downloads/predict movie rental duration/predict movie rental duration/rental_info.csv")
# View structure of dataset
str(rental)
unique(rental$special_features)
# Check for missing values
sapply(rental, function(x) sum(is.na(x)))
## Get the response variable from your data and add new variables
rental$rent_hours <- rental$rent_hours <- difftime(rental$return_date,
rental$rental_date,
units = "hours") # rent_hours variable
# Rent day of the week
rental$rent_dow <- as.factor(weekdays(rental$rental_date))
# Rent month of the year
rental$rent_month <- as.factor( format(rental$rental_date, "%m"))
# Target variable rent_length
rental$rent_length <- round(as.numeric(rental$rent_hours / 24), 2)
# Convert rent_hours to numeric
rental$rent_hours <- as.numeric(rental$rent_hours)
rental <- rental %>%
arrange(rental_date) %>%
mutate(
avg_rent_before = lag(cummean(rent_hours))
)
str(rental)
# deleted_scenes variable
rental$deleted_scenes = as.numeric(grepl("Deleted Scenes", rental$special_features))
# behind_the_scenes variable
rental$behind_the_scenes = as.numeric(grepl("Behind the Scenes", rental$special_features))
# Perform train-test split
set.seed(85)
splits <- initial_split(rental, prop = 0.8)
train <- training(splits)
test <- testing(splits)
# Check for correlation in the predictor variable
cor_matrix <- train %>%
select_if(is.numeric) %>%
select(-rent_length) %>%
cor()
cor_matrix
# Preprocess using tidymodels
rec <- recipe(rent_length ~ ., data = train) %>%
update_role(rental_date, return_date, special_features, new_role = "ID") %>%
step_rm(has_role("ID")) %>%
step_rm(rent_hours) %>% # to avoid leak since target variable was computed using rent_hours
step_nzv(all_predictors()) %>%
step_impute_mean(avg_rent_before) %>%    # replace NAs due to lag() used when creating the variable with the training-set mean
step_corr(all_numeric(), -all_outcomes(), threshold = 0.9) %>%
step_normalize(all_numeric(), -all_outcomes())
# train with prep() and apply with bake() to know how many predictors remaining
rec_prep <- prep(rec, training = train)
# Bake it to get the processed training tibble
train_baked <- bake(rec_prep, new_data = NULL)
# View its dimensions
dim(train_baked)
#  Compute maximum mtry parameter programmatically
max_mtry <- ncol(train_baked) - 1
max_mtry
# Predictor Importance
spec <- rand_forest ()%>%
set_mode("regression")%>%
set_engine("ranger", importance = "impurity")
# fit on train
model_1 <- spec %>%
fit(rent_length ~ ., data = train)
# View variable Importance
vip::vip(model_1)
# Model specification
model_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune())%>%
set_engine("ranger") %>%
set_mode("regression")
model_spec
rf_grid <- grid_random(
mtry(range = c(1, max_mtry)),
trees(range = c(500, 1200)),
min_n(range = c(1, 50)),
size = 25
)
set.seed(123)
cv_splits <- vfold_cv(train, v = 5)
rf_workflow <- workflow()%>%
add_model(model_spec)%>%
add_recipe(rec)
# Add a control that catches errors
ctrl <- control_grid(
verbose = TRUE,
catch   = TRUE
)
packageVersion("tune")
packageVersion("tidymodels")
find("control_grid")
install.packages("finetune")
library(finetune)
install.packages("finetune")
# Imports
library(dplyr)
library(rsample)
library(tidymodels)
library(lubridate)
library(caret)
install.packages("glmnet")
library(glmnet)
install.packages("dials")
install.packages("readr")
install.packages("finetune")
library(finetune)
library(dials)
library(readr)
install.packages("vip")
library(vip)
# Read the CSV file
rental <- read_csv("C:/Users/Anthony Ebuboh/Downloads/predict movie rental duration/predict movie rental duration/rental_info.csv")
# View structure of dataset
str(rental)
unique(rental$special_features)
# Check for missing values
sapply(rental, function(x) sum(is.na(x)))
## Get the response variable from your data and add new variables
rental$rent_hours <- rental$rent_hours <- difftime(rental$return_date,
rental$rental_date,
units = "hours") # rent_hours variable
# Rent day of the week
rental$rent_dow <- as.factor(weekdays(rental$rental_date))
# Rent month of the year
rental$rent_month <- as.factor( format(rental$rental_date, "%m"))
# Target variable rent_length
rental$rent_length <- round(as.numeric(rental$rent_hours / 24), 2)
# Convert rent_hours to numeric
rental$rent_hours <- as.numeric(rental$rent_hours)
rental <- rental %>%
arrange(rental_date) %>%
mutate(
avg_rent_before = lag(cummean(rent_hours))
)
str(rental)
# deleted_scenes variable
rental$deleted_scenes = as.numeric(grepl("Deleted Scenes", rental$special_features))
# behind_the_scenes variable
rental$behind_the_scenes = as.numeric(grepl("Behind the Scenes", rental$special_features))
# Perform train-test split
set.seed(85)
splits <- initial_split(rental, prop = 0.8)
train <- training(splits)
test <- testing(splits)
# Check for correlation in the predictor variable
cor_matrix <- train %>%
select_if(is.numeric) %>%
select(-rent_length) %>%
cor()
cor_matrix
# Preprocess using tidymodels
rec <- recipe(rent_length ~ ., data = train) %>%
update_role(rental_date, return_date, special_features, new_role = "ID") %>%
step_rm(has_role("ID")) %>%
step_rm(rent_hours) %>% # to avoid leak since target variable was computed using rent_hours
step_nzv(all_predictors()) %>%
step_impute_mean(avg_rent_before) %>%    # replace NAs due to lag() used when creating the variable with the training-set mean
step_corr(all_numeric(), -all_outcomes(), threshold = 0.9) %>%
step_normalize(all_numeric(), -all_outcomes())
# train with prep() and apply with bake() to know how many predictors remaining
rec_prep <- prep(rec, training = train)
# Bake it to get the processed training tibble
train_baked <- bake(rec_prep, new_data = NULL)
# View its dimensions
dim(train_baked)
#  Compute maximum mtry parameter programmatically
max_mtry <- ncol(train_baked) - 1
max_mtry
# Predictor Importance
spec <- rand_forest ()%>%
set_mode("regression")%>%
set_engine("ranger", importance = "impurity")
# fit on train
model_1 <- spec %>%
fit(rent_length ~ ., data = train)
# View variable Importance
vip::vip(model_1)
# Model specification
model_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune())%>%
set_engine("ranger") %>%
set_mode("regression")
model_spec
rf_grid <- grid_random(
mtry(range = c(1, max_mtry)),
trees(range = c(500, 1200)),
min_n(range = c(1, 50)),
size = 25
)
set.seed(123)
cv_splits <- vfold_cv(train, v = 5)
rf_workflow <- workflow()%>%
add_model(model_spec)%>%
add_recipe(rec)
# Add a control that catches errors
ctrl <- control_grid(
verbose = TRUE,
catch   = TRUE
)
find("control_grid")
# Imports
install.packages("workflows")
library(workflows)
listings <- read_csv("C:\Users\Anthony Ebuboh\Downloads\listings (1).csv")
listings <- read_csv("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
library(readr)
listings <- read_csv("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
file.exists("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
listings <- read_csv("C:\Users\Anthony Ebuboh\Downloads\listings (1).csv")
listings <- read_csv("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
file.info("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
Sys.info()["user"]
dir.exists("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
list.files("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv", all.files = TRUE)
file.info("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv")
listings <- read_csv("C:/Users/Anthony Ebuboh/Downloads/listings (1).csv/listings.csv")
colnames(listings)
library(sf)
list_geo <- st_as_sf(listings, coords = c("longitude","latitude"))
list_geo$geometry
colnames(list_geo)
lon_poly <- st_read("C:/Users/Anthony Ebuboh/Downloads/statistical-gis-boundaries-london (1)/statistical-gis-boundaries-london")
lon_poly <- st_read("C:/Users/Anthony Ebuboh/Downloads/statistical-gis-boundaries-london (1)/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")
lon_poly$geometry
lon_loop <- st_read("C:/Users/Anthony Ebuboh/Downloads/London-Loop-SWC-Walk-L24.kml", driver="KML")
sf::st_layers("C:/Users/Anthony Ebuboh/Downloads/London-Loop-SWC-Walk-L24.kml")
lon_loop <- st_read("C:/Users/Anthony Ebuboh/Downloads/London-Loop-SWC-Walk-L24.gpx", layer = "tracks")
lon_loop <- st_read("C:/Users/Anthony Ebuboh/Downloads/London-Loop-SWC-Walk-L24-trk.gpx", layer = "tracks")
lon_loop$geometry
lon_capital <- st_read("C:/Users/Anthony Ebuboh/Downloads/Capital-Ring-SWC-Walk-L23.gpx")
lon_capital$geometry
st_layers("C:/Users/Anthony Ebuboh/Downloads/Capital-Ring-SWC-Walk-L23.gpx")
lon_capital <- st_read("C:/Users/Anthony Ebuboh/Downloads/Capital-Ring-SWC-Walk-L23.gpx", layer = "routes")
lon_capital <- lon_capital[1, ]  # Selects the first (main) route
lon_capital$geometry
View(list_geo)
View(list_geo)
View(listings)
library(tune)
library(workflowsets)
library(bestNormalize)
library(tidyverse)
install.packages("tidyverse")
# Read the CSV file
rental <- read_csv("C:/Users/Anthony Ebuboh/Downloads/predict movie rental duration/predict movie rental duration/rental_info.csv")
install.packages("hardhat")
install.packages("hardhat")
install.packages("workflows")
install.packages("vip")
install.packages("dials")
install.packages("readr")
install.packages("finetune")
install.packages("glmnet")
install.packages("tune")
install.packages("bestNormalize")
install.packages("workflowsets")
library(tune)
library(workflowsets)
library(bestNormalize)
library(tidyverse)
library(dplyr)
library(rsample)
library(tidymodels)
install.packages("tidymodels")
library(tidymodels)
# Read the CSV file
rental <- read_csv("C:/Users/Anthony Ebuboh/Downloads/predict movie rental duration/predict movie rental duration/rental_info.csv")
# View structure of dataset
str(rental)
